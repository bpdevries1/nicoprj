Logboek
=======
#Gets MD5 hash
 proc md5 {string} {
     #tcllib way:
     package require md5
     string tolower [::md5::md5 -hex $string]

     #BSD way:
     #exec -- md5 -q -s $string
 
     #Linux way:
     #exec -- echo -n $string | md5sum | sed "s/\ *-/\ \ /"

     #Solaris way:
     #lindex [exec -- echo -n $string | md5sum] 0

     #OpenSSL way:
     #exec -- echo -n $string | openssl md5
 }
 
 
package require struct::list
package require fileutil

[::fileutil::cat $filename]

# wel vraag of deze met grote files om kan gaan.
::md5::md5 -hex [::fileutil::cat make-svn-ontdubbel.tcl]

# je kan ook channel of file opgeven, dus dit lijkt wel de eerste manier.
::md5::md5  ? -hex ?  [ -channel channel | -file filename | string ]

[2012-01-19 22:25:52] met backup ook iets gedaan dat je checkt of er wat veranderd is. Hele harddisk duurt erg lang, en
                      zal nu met md5 nog langer duren. Dit is dus niet iets wat je veel wilt doen.
[2012-01-19 22:26:55] bijhouden waar je bent klinkt wel noodzakelijk, maar eerst zonder beginnen.
[2012-01-19 22:27:15] dirs doorlopen wel net als backup? bv niet symlinks doorlopen. Mss ook ignorefiles toepassen.
[2012-01-20 08:28:37] foutmelding in dir /home/nico/oltest, zie onder.
[2012-01-21 19:13:22] moet SQL escapen dus, jammer.

[21-01-12 14:56:34] [catalogdisk.tcl] [debug] handling: /home/nico/oltest/out/Personal/Verwijderde items/Adressen/Collega's
[2012-01-22 01:20:39] bugs opgelost, nu /home/nico goed ingelezen, totaal 22.39-00.55=2:16. 
   102.872 files, totalsize=76 550 712 988.0 ofwel 76GB.
[2012-01-22 01:25:09] aan de andere kant met du -H blijkt in / 115G totaal te zijn, met 84GB gebruikt, zou meeste dus in /home/nico
   zitten.
[2012-01-22 01:26:44] dubbele dingen op andere lokatie dan mijn home-dir zijn niet zo waarschijnlijk, wel dingen in /opt, /var
   /etc
[2012-01-22 01:27:55] nu eerst /media/nas aanzetten.
[2012-01-22 01:28:55] ook bv /aaa dirs staan ofwel in mij home, ofwel op /media/nas/aaa.
[2012-01-22 01:29:54] waar ik overal books/ict heb, is ook wel leuk te weten: /media/nas, laptop, dropbox. Als /media/nas
   de bron is, en de andere 2 puur omdat je er dan beter bijkan, is het goed. Op laptop zou nog meer kunnen staan dan in
   dropbox, omdat daar meer ruimte is.
[2012-01-28 22:27:04] heb ook nog diskusage van /media/nas?  
[2012-01-28 22:36:50] kan keuze zijn om films (bv die ik al gezien heb, niet per se nogeens wil zien) alleen op 2TB te zetten,
                      dan idd geen backup.
[2012-01-28 22:48:13] /media/nas/media/PodNova verwijderd, Bommel spul, al in Diversen/Bommel.
[2012-01-28 23:08:09] Op /media/nas nu 105 GB vrij.
[2012-01-29 11:23:20] attach database "c-root.db" as croot; dus wel quotes rond dbfilename.
[2012-01-29 11:27:06] create index ix_filename on files (filename); paar minuten geleden gestart, heeft wel tijd nodig.
[2012-01-29 11:29:40] index maken klaar.
[2012-01-29 11:30:31] ook indexen op size en md5sum nodig? Als ik size heb, is sum mss niet meer nodig.
[2012-01-29 11:30:54] create index ix_size on files (filesize); ok, weet niet hoe lang het duurt.
[2012-01-29 12:15:18] test: 2 grote files met dezelfde grootte, en daarna dezelfde md5?
select f1.folder, f1.filename, f1.filesize, f1.md5sum, f2.folder, f2.filename, f2.filesize, f2.md5sum 
from files f1, files f2
where f1.filesize > 10000000 
and f1.filesize = f2.filesize
and f1.id < f2.id limit 10;
[2012-01-29 12:19:20] filesize wordt zo als string bepaald, dus niet goed. Heb filesize ook als varchar, mss iets mee te maken.
select f1.folder, f1.filename, f1.filesize, f1.md5sum, f2.folder, f2.filename, f2.filesize, f2.md5sum 
from files f1, files f2
where f1.filesize+1 > 10000000 
and f1.filesize = f2.filesize
and f1.id < f2.id limit 10;
[2012-01-29 12:28:44] met index erbij is /media/nas.db nu 461MB.
[2012-01-29 12:29:34] copy van media-nas.db naar hd-all.db, en dan de andere 3 erbij. Gaat mss traag omdat index er al opstaat.
[2012-01-29 12:32:31] bij insert-into niet de id meenemen, zou overlappend kunnen zijn, dus deze opnieuw bepalen.

attach database "c-root.db" as croot;
select count(*) from files; 
; 1636337
select count(*) from croot.files;
; 436521
; verwachting straks samen is dan: 1636337+436521= 2072858 (op unix met expr commando, wel spaties)
; dus ruim 2 miljoen bestanden/records

insert into files (folder, filename, filedate, filesize, md5sum, lastchecked)
select folder, filename, filedate, filesize, md5sum, lastchecked
from croot.files;

[2012-01-29 12:39:08] bovenstaande insert paar minuten geleden gestart.
[2012-01-29 12:44:17] nog steeds bezig...
[2012-01-29 12:47:38] mss gaat het beter als de indexen er eerst af zijn...
[2012-01-29 12:53:53] nog bezig, vind het eigenlijk te lang duren
[2012-01-29 12:55:54] huidige size is al meer dan de som van de ouden, kan aan index liggen.
[2012-01-29 12:56:22] is nu klaar, zo'n 20 minuten over gedaan.
[2012-01-29 12:56:48] andere 2 zijn kleiner, zou totaal dan binnen uur moeten lukken, dus maar doen.
[2012-01-29 12:57:07] 

attach database "old-drives.db" as old;
select count(*) from old.files;
; 294742 records in old.files, ruim minder dan in c-root.

insert into files (folder, filename, filedate, filesize, md5sum, lastchecked)
select folder, filename, filedate, filesize, md5sum, lastchecked
from old.files;

[2012-01-29 12:58:21] bovenstaande zo starten...
[2012-01-29 12:58:35] is gestart.
[2012-01-29 13:22:34] alweer een tijdje klaar
[2012-01-29 13:23:05] even een count, verwachting: 2072858 + 294742 = 2367600, result=2367600, lijkt goed.


attach database "home-nico.db" as home;
select count(*) from home.files;
; 102872

insert into files (folder, filename, filedate, filesize, md5sum, lastchecked)
select folder, filename, filedate, filesize, md5sum, lastchecked
from home.files;
[2012-01-29 13:24:18] net gestart.
[2012-01-29 13:40:54] klaar, count nu (verwacht: 2367600 + 102872 = 2470472): 2470472, dus goed.
[2012-01-29 13:42:00] totale size van .db: 661.805.056, ofwel 661MB, past nog net op een CD.

[2012-01-29 13:43:04] even select weer proberen:
select f1.folder, f1.filename, f1.filesize, f1.md5sum, f2.folder, f2.filename, f2.filesize, f2.md5sum 
from files f1, files f2
where f1.filesize+1 > 10000000 
and f1.filesize = f2.filesize
and f1.id < f2.id limit 10;
[2012-01-29 13:43:50] staat me wel iets bij dat ik dubbelcheck al eerder heb gedaan, ook met bepalen welke
                      houden en welke weg.
[2012-01-29 13:45:47] als ik dit in een script doe, dan niet meteen alle resultaten in een resultset, maar per stukje doen.
[2012-01-29 14:16:35] voor de zekerheid hd-all.db naar hd-all-orig.db gekopieerd.
[2012-01-29 14:42:27] binnen Tcl wordt a) niets gevonden en b) duurt het zoeken lang. Dezelfde query in sqlite3 gaat vlot.

% db eval "select * from files where filename='545.mp3' limit 10"
30163 {/media/nas/media/PodNova-Downloads/PodNova Clippings vreeze42} 545.mp3 {2009-03-02 10:45:30} 20227222 7deec2031765b6706292697d9deac5a7 {2012-01-22 04:06:48} 31827 /media/nas/media/Diversen/Bommel 545.mp3 {2009-03-02 10:45:00} 20227222 7deec2031765b6706292697d9deac5a7 {2012-01-22 07:41:38}

db eval "select * from files f1, files f2 where f1.filename='545.mp3' and f2.filename='545.mp3' limit 10"
# ok
db eval "select * from files f1, files f2 where f1.filename='545.mp3' and f2.filename='545.mp3' and f1.id < f2.id limit 10"
# ook goed
db eval "select * from files f1, files f2 
where f1.filename='545.mp3' 
and f2.filename='545.mp3' 
and f1.id < f2.id limit 10"
# ook goed
db eval "select * from files f1, files f2 
where f1.filename='545.mp3' 
and f1.filesize+1 > $minsize
and f1.filesize = f2.filesize
and f2.filename='545.mp3' 
and f1.id < f2.id limit 10"
# leeg, wel snel

db eval "select * from files f1, files f2 
where f1.filename='545.mp3' 
and f1.filesize = f2.filesize
and f2.filename='545.mp3' 
and f1.id < f2.id limit 10"
# goed

db eval "select * from files f1, files f2 
where f1.filename='545.mp3' 
and f1.filesize+1 > $minsize
and f2.filename='545.mp3' 
and f1.id < f2.id limit 10"
# ook goed, wel vaag.

# dan bij vorige alleen de gelijkheid van filesizes erbij:
db eval "select * from files f1, files f2 
where f1.filename='545.mp3' 
and f1.filesize+1 > $minsize
and f2.filename='545.mp3' 
and f1.filesize = f2.filesize
and f1.id < f2.id limit 10"
# is weer leeg

db eval "select * from files f1, files f2 
where f1.filename='545.mp3' 
and f1.filesize+1 > $minsize
and f2.filename='545.mp3' 
and f1.filesize+1 = f2.filesize+1
and f1.id < f2.id limit 10"
# dan weer wel!

[2012-01-29 14:55:08] lijkt dat kolom in numeric mode gaat, en dan dus steeds zo benaderd moet worden.
[2012-01-29 14:55:27] proberen in de script-query. Sowieso wijken de sqlite3 en tcl versies af.
[2012-01-29 14:57:14] in script lijkt het weer niet goed.
[2012-01-29 15:01:48] kan cast(filesize as integer) gebruiken.
[2012-01-29 15:04:30] query wordt nu wel gedaan in tcl, maar duurt nog lang, lijkt alsof eerst hele resultset bepaald wordt,
                      en pas dan de eerste 10 teruggegeven.
[2012-01-29 15:05:02] wel cast in de query gezet, lijkt netter dan +1.
[2012-01-29 15:07:17] duurt allemaal erg lang in tcl, ook doorlopen van de resultset.
[2012-01-29 15:10:09] wel benieuwd hoe snel dit op laptop gaat.
[2012-01-29 15:12:15] gestart met kopieren naar NAS...
[2012-01-29 15:19:24] kopieren inmiddels klaar. nu naar laptop.
[2012-01-29 15:20:01] gestart met copy-to-laptop.
[2012-01-29 15:22:19] ook alweer klaar.
[2012-01-29 15:23:39] Clj-scrabble ook maar eens gestopt, had hier out-of-memory, dus mss nog wel veel in gebruik.
[2012-01-29 15:23:57] sluiten hiervan duurde ook wel eventjes.
[2012-01-29 15:26:58] uitvoeren op laptop lijkt stuk vlotter, nu nog eens op ubuntu.
[2012-01-29 15:55:56] ubuntu nog steeds traag
[2012-01-29 15:56:02] wil niet 'per ongeluk' file deleten uit backup-current of laptop of dropbox omdat bv /media/nas/boeken
                      als source is aangewezen, en later bepalen dat het toch wel handig is de file op dropbox en/of laptop
                      te hebben.
[2012-01-29 15:57:03] als file in dropbox zit, hoeft het niet meer ook op laptop, staat het dan toch al.
[2012-01-29 15:57:53] files in dropbox kom je sowieso 2x tegen, 1 keer via /home/nico, 1x via laptop. Deze dan beide bewaren.
[2012-01-29 15:58:33] even vraag dus of een single waardering genoeg is, of dat je alles 1-op-1 moet vergelijken.
[2012-01-29 15:59:29] eigenlijk def je groepen, die in principe niet mogen overlappen. Dan per groep-combi bepalen wat er
[2012-01-29 15:59:58] moet gebeuren.
[2012-01-29 16:00:11] bv binnen /media/boeken is de meest gedetailleerde folder beter, want specifieker?
[2012-01-29 16:01:45] kan ook zeggen dat ik eerst current-backup en dropbox buiten beschouwing laat. Dan liefst al in de query
[2012-01-29 16:02:16] doen, evt pas in tcl verwerking. Kan tcl-functie opnemen in query omgeving, mss hierdoor ook wel trager.
[2012-01-29 16:23:53] dropbox en laptop/bieb als een soort cache te zien, current-backup op een bepaalde manier ook.
[2012-01-29 16:24:26] dit betekent dat er ergens een bron moet zijn van de file in de cache: if so, ok, if not: vaag: ofwel
                      file moet naar de source, ofwel file mag uit de cache. 2 files uit cache vergelijken heeft niet zoveel
                      zin. Nuance: 2 files uit verschillende caches vgl heeft geen zin, maar 2 uit dezelfde wel: kan 1 in principe weg.
[2012-01-29 16:27:28] deze soorten (bron, backup, cache) en evt de specifieke (cache-dropbox, cache-laptop) al in de database,
                      lijkt efficienter.
[2012-01-29 16:30:59] dropbox kan zowel een cache zijn als een bron. boeken als cache, bv current-docs als dropbox, zoals volley.                      
[2012-01-29 17:18:19] nu al aardig wat in de ingore-list, hierdoor duurt het best lang voordat 'ie met de eerste begint, hierna 
                      gaat het wel. Kan evt de 'ignore' records verwijderen of markeren. 1 extra kolom lijkt toch handig, flexibel
                      in te vullen. Kan ook wel (volgens normaliseren) een extra tabel/join doen, maar daar wordt het ook niet sneller
                      van.
[2012-01-29 17:25:37] kolom erbij lijkt vrij gemakkelijk. Hoef dus ook niet 1 algemene te doen, kan evt een voor ignore, een voor type, 
                      een voor subtype.

alter table files add column filesize_int integer;
alter table files add column loc_type;
alter table files add column loc_detail;

[2012-01-29 18:52:54] houd laptop versie van de hd-all.db nu als bron, hier kolommen bij en vullen, testen van de int-kolom.


[2012-02-26 22:40:09] Tijd geleden, weer verder: nu alle groepen gedef'ed, vrij defensief, ook /home/nico als source genoemd. Hierna alles vanaf 10MB afgehandeld.
[2012-02-26 23:31:55] bezig met dingen vanaf 1MB, nog best veel, ook paar extra dingen def-en in alter.txt
[2012-02-26 23:32:14] git exe's lijken allen dezelfde inhoud te hebben, alleen filenaam is anders. Hierdoor heel veel checks, duurt lang.
[2012-03-01 22:37:29] Nog steeds met de git checks bezig, zijn in util\msysgit 100 files, totaal deze dit is 1.2 GB.
[2012-03-01 22:38:21] wel ook archief-type geintroduceerd.
[2012-03-01 22:38:37] nog bezig dus met files > 1MB.
[2012-03-01 22:49:18] git dingen nu wel gehad, nu ook nog veel muziek. Doe het in batches van 10.000, zijn er best veel van.
[2012-03-01 22:50:11] Seinfeld gekopieerd van /Downloads naar /media/nas, hierdoor op /media/nas nog maar 39GB beschikbaar.
[2012-03-03 15:50:41] net weer gestart, maar blijken niet meer te zijn.
[2012-03-03 16:27:19] Eens kijken naar old backups, of hier nog files > 1 of 10 MB instaan.

select * from files
where loc_type = 'oldbackup'
and filesize_int > 10000000
limit 10;

[2012-03-03 17:48:39] Zijn er dus zeker, nogal wat werk om te checken. 

Aantal categorieen: 
* eigen data: wel houden, komt ook wel voor, bv WM_data van Windows Mobile, bv logs van Naty.
* util en program files: eigenlijk meteen weg, voor de vorm nog even kijken, wil toch meer de install versies.
* cruiseresults: lastige
* perftoolset/dist: zou zeggen dat het weg mag.

[2012-03-04 16:48:03] make-del-doubles aan het draaien met oldbackup minsize = 0 en limit is 100,000. Zal vast lang duren, nu ook
  nog geen zicht hoeveel bestanden het zijn. Maar delete van d:\develop is ook nog bezig.
[2012-03-04 18:18:29] minsize = 0 werkt niet, veel bestanden van 1 en 2 bytes, die gelijk zijn aan heel veel andere bestanden, daardoor
  weer een cross-product, vgl git.exe files. In theorie zijn deze bestandjes (bv in git) nog belangrijk ook).
[2012-03-04 18:19:26] dus nu met minsize =10 gedaan. Deze is veel sneller klaar, wel 91MB. Hierin nog veel exception.count bestanden,
  maar deze toch maar deleten. Op linux ging dit deleten wel, op windows heel lang bezig, gestopt. Size=11.
[2012-03-04 19:08:47] dan nog maar eens met minsize=15... Nog steeds veel dezelfde, en weer kwadratisch. Dit lijkt niet een werkbare
  manier. Alternatieven:
  * ook naar filenaam kijken: helpt weinig, de files heten allemaal hetzelfde, alleen dirs zijn anders.
  * bij media-old is aanwezigheid van 1 andere file genoeg, je hoeft niet meer te zoeken:
    * in query deze dubbelen al weg: file2 niet meejoinen en een select distinct doen. Of een where exists.
    * file meteen weg, bij volgende kijken of file nog bestaat: dan werkt huidige opzet niet, met laptop en ubuntu, plus het is later checken, 
      dus langzamer.
    * iets met ineens alle dubbelen van een size/MD5 zoeken: mogelijk deze voor latere checks, niet met oldbackups.
[2012-03-04 19:39:23] door distinct duurt het idd langer. Doorlopen resultset duurt ook langer, mogelijk omdat delen van de result nog opgehaald
  worden? Distinct is impliciet een sort, en in combi met limit is dit niet ideaal.
[2012-03-04 19:54:16] Kan ook zijn dat het de backup is, die vanaf 19:32 loopt, deze gekilled nu.
[2012-03-04 19:55:43] nogmaals uitvoeren met limit 100,000, deze begint nu wel meteen, query result nog in de cache ergens?
[2012-03-05 09:03:18] delete-files heeft lang geduurt, na 12.00 uur klaar.
[2012-03-05 09:03:36] net weer nieuwe make-delete gestart.
[2012-03-05 09:04:41] delete-files: heb begin transaction/commit helemaal aan het begin/eind. Toch af en toe deelcommit? Op windows gedaan, moet nog gecommit etc.
[2012-03-05 12:02:06] nu paar keer make-delete/delete gedaan, zou eens count op DB kunnen doen hoeveel er in oldbackup staat, zowel #files als #MB.
[2012-03-05 12:03:08] delete inmiddels weer klaar op linux, maar nog niet op windows. Waarom duurt het hier zo lang, bestanden staan hier niet eens, dus delete hoeft
  niets te doen.
[2012-03-05 12:54:52] Delete op windows ook klaar, count op oldbackup: 

select sum(filesize_int) size, count(*) cnt
from files where loc_type = 'oldbackup';

14125651112|229111

[2012-03-05 12:56:58] 229.000 bestanden is te doen in batches van 100,000. Size is nu nog:14.125.651.112, ofwel 14GB, wel de moeite.
[2012-03-05 14:04:29] weer een run gedaan, nu nog over: 

13924279697|221185

[2012-03-05 18:46:45] Dit is wel apart, maar kleine 8.000 minder. Klopt wel met grootte van w:\rm-files.txt, nog eens met filesize_int >= 15:

select sum(filesize_int) size, count(*) cnt
from files where loc_type = 'oldbackup' and filesize_int >= 15;
13924251076|214527

ofwel bijna net zoveel.

[2012-03-05 18:56:44] mss geldt limit voordat met distinct de boel wordt ingedikt. Nu nog eens starten met limit=10 miljoen, kan wel zijn dat file dan rond 1GB wordt, moet kunnen.
[2012-03-05 18:58:14] gestart.
[2012-03-05 19:09:49] lijkt klaar nu, maar heeft niets gedaan, w:\rm-files is leeg. Betekent dit dat de rest allemaal uniek is? Nergens anders te vinden? En wat moet ik er dan mee?

[2012-03-08 21:01:12] Eerst alle lege dirs verwijderen. Kan zijn dat dirs alleen subdirs hebben, deze moeten dan ook weg.

del /sxyz abcxyz*

[2012-03-08 21:04:35] Eerst gedaan op dellLaptop. Duurt wel even.

sqlite> select * from files where folder like '/media/nas/backups/DellLaptop/d/B
orland%' limit 10;
sqlite>

[2012-03-08 21:10:44] In Borland folder dus geen files meer, wel veel lege dirs. Deze straks allemaal weg?
[2012-03-08 21:12:12] Duurt best lang.
[2012-03-08 21:14:46] Gestopt en nu op de hele backup dir gedaan, zal wel lang duren, later maar eens kijken.
[2012-03-08 23:40:57] Al een tijdje klaar, kan wel opschonen, wel last van kleine files, deze toch ook maar allemaal weg, kan nooit veel in staan. Hierna weer lege dirs weghalen, duurt wel weer lang, maar ja.


